{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTSFmCGgx6YO",
        "outputId": "a0a023b5-e6ce-4f30-9d1c-c68da8eeff95"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/NLP_Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGQd8uXox7Gn",
        "outputId": "a85e669e-6b09-42f3-d539-02c15a4294a0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icD0B7_IyFPf",
        "outputId": "fb693f0a-fbfe-40e0-9743-5896f6c59ec4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GoogleNews-vectors-negative300.bin  words_147mb.txt   words_569.5mb.txt\n",
            "NLP_PRO.ipynb                       words_1.73gb.txt  words_62mb.txt\n",
            "vocabulary.pkl                      words_2.02gb.txt  words_870mb.txt\n",
            "\u001b[0m\u001b[01;34mwebbase_all\u001b[0m/                        words_2.32gb.txt  words_91mb.txt\n",
            "words_1.45gb.txt                    words_300mb.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !tar zxvf umbc_webbase_corpus.tgz"
      ],
      "metadata": {
        "id": "UCtm9LvXie45"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required libraries"
      ],
      "metadata": {
        "id": "jT9VDDn6VjTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "5hqHSK9sVi5i"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset_folder_path = os.path.join(os.getcwd(), 'webbase_all')"
      ],
      "metadata": {
        "id": "oySXc8poValt"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count = 0\n",
        "# txt_count = 0\n",
        "# txt_files = []\n",
        "\n",
        "# pos_tagged_count = 0\n",
        "\n",
        "# # Iterate directory\n",
        "# for path in os.listdir(dataset_folder_path):\n",
        "    \n",
        "#     file_path = os.path.join(dataset_folder_path, path)\n",
        "    \n",
        "#     # check if current path is a file\n",
        "#     if os.path.isfile(file_path):\n",
        "#         count += 1\n",
        "\n",
        "#     if file_path.endswith('.txt'):\n",
        "#         txt_count += 1\n",
        "#         txt_files.append(file_path)\n",
        "    \n",
        "#     else:\n",
        "#         os.remove(file_path)\n",
        "    \n",
        "# print(\"Number of files: \", count)\n",
        "# print(\"Number of text files: \", txt_count)\n",
        "# print(\"Number of pos tagged files: \", pos_tagged_count)"
      ],
      "metadata": {
        "id": "ulITZ7yzQNxs"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # import the necessary libraries\n",
        "# import nltk\n",
        "# import string\n",
        "# import re\n",
        "\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "# stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "uB-eLH978_CK"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing sentences**"
      ],
      "metadata": {
        "id": "PbQ_5DO4CbpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess(sentence):\n",
        "#     # convert to lowercase\n",
        "#     sentence = sentence.lower()\n",
        "\n",
        "#     # remove numbers\n",
        "#     sentence = re.sub(r'\\d+', '', sentence)\n",
        "\n",
        "#     # remove punctuations\n",
        "#     # anything that isnâ€™t an alphanumeric character or whitespace is replaced with a blank string\n",
        "#     sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "\n",
        "#     # remove stopwords\n",
        "#     tokens = word_tokenize(sentence)\n",
        "#     result = [i for i in tokens if not i in stop_words]\n",
        "#     sentence = (\" \").join(result)\n",
        "\n",
        "#      # remove whitespace\n",
        "#     sentence = sentence.strip()\n",
        "\n",
        "#     return sentence "
      ],
      "metadata": {
        "id": "wU5mbz6qNZuz"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_file = open(\"words.txt\", \"w\")\n",
        "# result_lines = []\n",
        "\n",
        "# for file_path in txt_files[:2]:\n",
        "#     f = open(file_path, 'r')\n",
        "\n",
        "#     for line in f.readlines():\n",
        "#         result = preprocess(line)\n",
        "\n",
        "#         if (result != \"\"):\n",
        "#             result_lines.append(result+\"\\n\")\n",
        "\n",
        "# output_file.writelines(result_lines)\n",
        "# output_file.close()"
      ],
      "metadata": {
        "id": "T9K4e_APcTwu"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset file"
      ],
      "metadata": {
        "id": "58neYPuiCj5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cmCqMtuXlAp",
        "outputId": "cb83f28e-e173-4eb8-b504-90400cc12d6d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GoogleNews-vectors-negative300.bin  words_147mb.txt   words_569.5mb.txt\n",
            "NLP_PRO.ipynb                       words_1.73gb.txt  words_62mb.txt\n",
            "vocabulary.pkl                      words_2.02gb.txt  words_870mb.txt\n",
            "\u001b[0m\u001b[01;34mwebbase_all\u001b[0m/                        words_2.32gb.txt  words_91mb.txt\n",
            "words_1.45gb.txt                    words_300mb.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_dataset_path = \"words_62mb.txt\"\n",
        "# words_dataset_path = \"words_300mb.txt\""
      ],
      "metadata": {
        "id": "6VhRwU1Bm4ws"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_dataset_file = open(words_dataset_path, \"r\")\n",
        "\n",
        "sentences = words_dataset_file.readlines()\n",
        "\n",
        "words_dataset_file.close()"
      ],
      "metadata": {
        "id": "hevL3HT9FhAU"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import pickle\n",
        "\n",
        "# Load pre-trained word embeddings (e.g. Word2Vec or GloVe)\n",
        "embeddings_path = \"GoogleNews-vectors-negative300.bin\"\n",
        "embeddings_model = gensim.models.KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n",
        "\n",
        "# Generate a vocabulary of words and their corresponding embeddings\n",
        "vocab = {}\n",
        "for sentence in sentences:\n",
        "    for word in sentence.split():\n",
        "        if word not in vocab and word in embeddings_model:\n",
        "            vocab[word] = embeddings_model[word]\n",
        "\n",
        "# Save the vocabulary to a file\n",
        "vocab_path = \"vocabulary.pkl\"\n",
        "with open(vocab_path, \"wb\") as f:\n",
        "    pickle.dump(vocab, f)"
      ],
      "metadata": {
        "id": "pk67By608_wu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YougiFuV9fCe",
        "outputId": "c1a3168f-bcb7-4d64-cc4a-757a748cabfc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def pmi(word1, word2, sentences):\n",
        "    pmi_scores = []\n",
        "\n",
        "    for sentence_str in sentences:\n",
        "        sentence = sentence_str.split()\n",
        "        \n",
        "        # Count the number of occurrences of each word in the sentence\n",
        "        word_counts = Counter(sentence)\n",
        "\n",
        "        # Check if word1 and word2 occur in the sentence\n",
        "        if word1 not in word_counts or word2 not in word_counts:\n",
        "            continue\n",
        "\n",
        "        # Calculate the total number of words in the sentence\n",
        "        total_words = len(sentence)\n",
        "\n",
        "        # Calculate the number of times word1 and word2 co-occur in the sentence\n",
        "        cooccurrences = sum(1 for i in range(len(sentence)-1) if word1 in sentence[i:] and word2 in sentence[i:])\n",
        "\n",
        "        # Calculate the PMI between word1 and word2 for this sentence\n",
        "        pmi_score = np.log2((cooccurrences * total_words) / (word_counts[word1] * word_counts[word2]))\n",
        "        \n",
        "        # Append the PMI score to the list of scores\n",
        "        pmi_scores.append(pmi_score)\n",
        "\n",
        "    # Calculate the average PMI score over all sentences\n",
        "    if pmi_scores:\n",
        "        return sum(pmi_scores) / len(pmi_scores)\n",
        "    else:\n",
        "        return 0.0\n",
        "    \n",
        "\n",
        "\n",
        "def generate_candidate_hypernyms(target_word, sentences, vocab, num_hypernyms=5, pmi_threshold=0.0):\n",
        "    # Find all sentences that contain the target word\n",
        "    target_sentences = [s for s in sentences if target_word in s.split()]\n",
        "\n",
        "    # Find words that frequently co-occur with the target word\n",
        "    cooccurring_words = set()\n",
        "    for target_sentence in target_sentences:\n",
        "        sentence_words = set(target_sentence.split())\n",
        "        cooccurring_words.update(sentence_words)\n",
        "    cooccurring_words.discard(target_word)\n",
        "\n",
        "\n",
        "    # Calculate the PMI between the target word and each candidate hypernym\n",
        "    pmi_scores = [(w, pmi(target_word, w, target_sentences)) for w in cooccurring_words]\n",
        "\n",
        "    # Filter out candidate hypernyms with low PMI scores\n",
        "    pmi_filtered = [(w, score) for (w, score) in pmi_scores if score > pmi_threshold]\n",
        "\n",
        "    # Sort the candidate hypernyms by descending PMI score\n",
        "    sorted_hypernyms = sorted(pmi_filtered, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Return the top N candidate hypernyms\n",
        "    top_hypernyms = [w for (w, score) in sorted_hypernyms[:num_hypernyms]]\n",
        "\n",
        "    # Calculate the cosine similarity between the target word and each candidate hypernym\n",
        "    embeddings = []\n",
        "    for w in [target_word] + top_hypernyms:\n",
        "        if w in vocab:\n",
        "            embeddings.append(vocab[w])\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "    similarities = similarity_matrix[0][1:]\n",
        "    \n",
        "    # Sort the candidate hypernyms by descending cosine similarity\n",
        "    sorted_hypernyms = [w for _, w in sorted(zip(similarities, top_hypernyms), reverse=True)]\n",
        "    \n",
        "    return sorted_hypernyms"
      ],
      "metadata": {
        "id": "jfYgZNFh9r2S"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "target_word = 'apple'\n",
        "\n",
        "candidate_hypernyms = generate_candidate_hypernyms(target_word, sentences, vocab)\n",
        "print(candidate_hypernyms)"
      ],
      "metadata": {
        "id": "HLhM-m1kXhWA"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bV0NAQWG6dp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}